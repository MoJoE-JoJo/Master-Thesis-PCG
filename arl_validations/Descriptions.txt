5_arl:
Smaller networks, ext/int reward scaling 1/20 and min+max level length in observation

6_arl:
learning rate 1e-5

7_arl:
Observation with int grid instead of single id

8_arl:
New reward structure using only simulation when done with generating level

9_arl:
Rerun of 5_arl, where loading is fixed to work with the new version

10_arl:
same as 8 but with internal = 10 and external = 1

11_arl:
Binary reward structure. If all constraints met, is equal to solver return times aux, else it gets -2500 for each of the 4 unmet constraints
Min length has also been set to 4
It also switches between aux_inputs 10 times more often

12_arl:
Same as 10, vectorized with 10 pcg environments that use the same solver environment (should be properly reset), and a min level length of 4

13_arl:
same as 10, but min level length of 4

14_arl:
Using an external reward based on mapping the winrate and aux-inputs of the agent to rewards
Also is unvectorized
Also has min-length of 10

15_arl:
Same as 14, but instead of mapping winrate and aux-input to rewards it maps solver-agent avg-rewards and aux-input to external reward


16_arl:
Same as 14_arl, but external reward scaled to 0.8 instead of 1, such it never can give a higher reward than getting a reward directly from the solver-agent

---------- Baseline experiments ----------
Experiments meant to find the baseline parameters and the edges of what, or whether it, is possible 
0_arl:
Generator with winrate-map reward, but not trained at all

17_arl:
Same as 14, winrate-reward map, but with only training with a single aux-input (aux=1)

18_arl:
Same as 17, but with an internal reward of 0, to check whether it can learn based on the winrate with the current intervals between updates (winrate-map should correspond directly to the desired performance of ht solver at  specific aux-input values)
(There is no reason to run with aux=0.5 as still with aux 1 it does not reach the target and just reaches 0.5 winrate)

19_arl:
Same as 18, but with updating the generator network every 16 steps instead of 32 steps


More things to try:
Smaller learning_rate (Done)
Observations using the tile-grid instead of the slice id (Done)
New reward structure running simulation on the terminal action instead of using a performance map
Different ratio between internal and external reward
Using vectorized generator environment
Using lower min-level-length

