5_arl:
Smaller networks, ext/int reward scaling 1/20 and min+max level length in observation

6_arl:
learning rate 1e-5

7_arl:
Observation with int grid instead of single id

8_arl:
(bad, used solver action for last action)
New reward structure using only simulation when done with generating level

9_arl:
(bad, used solver action for last action)
Rerun of 5_arl, where loading is fixed to work with the new version

10_arl:
(bad, used solver action for last action)
same as 8 but with internal = 10 and external = 1

11_arl:
(bad, used solver action for last action)
Binary reward structure. If all constraints met, is equal to solver return times aux, else it gets -2500 for each of the 4 unmet constraints
Min length has also been set to 4
It also switches between aux_inputs 10 times more often

12_arl:
(bad, used solver action for last action)
Same as 10, vectorized with 10 pcg environments that use the same solver environment (should be properly reset), and a min level length of 4

13_arl:
(bad, used solver action for last action)
same as 10, but min level length of 4

14_arl:
(bad, used solver action for last action)
Same as 10, but using an external reward based on mapping the winrate and aux-inputs of the agent to rewards
(So internal reward scaled by 10)
Also is unvectorized
Also has min-length of 10

15_arl:
(bad, used solver action for last action)
Same as 14, but instead of mapping winrate and aux-input to rewards it maps solver-agent avg-rewards and aux-input to external reward


16_arl:
(bad, used solver action for last action)
Same as 14_arl, but external reward scaled to 0.8 instead of 1, such it never can give a higher reward than getting a reward directly from the solver-agent

---------- Baseline experiments ----------
Experiments meant to find the baseline parameters and the edges of what, or whether it, is possible 
0_arl:
(bad, used solver action for last action, should not be a problem here)
Generator with winrate-map reward, but not trained at all (same as 14)

17_arl:
(bad, used solver action for last action)
Same as 14, winrate-reward map, but with only training with a single aux-input (aux=1)

18_arl:
(bad, used solver action for last action)
Same as 17, but with an internal reward of 0, to check whether it can learn based on the winrate with the current intervals between updates (winrate-map should correspond directly to the desired performance of the solver at specific aux-input values)
(There is no reason to run with aux=0.5 as still with aux 1 it does not reach the target and just reaches 0.5 winrate)

19_arl:
(bad, used solver action for last action)
Same as 18, but with updating the generator network every 16 steps instead of 32 steps

20_arl:
(bad, used solver action for last action)
Same as 18, but with updating the generator network every 64 steps instead of 32 steps

21_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as 17, but with updating the generator network every 16 steps instead of 32 steps

22_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as 17, but with updating the generator network every 64 steps instead of 32 steps


23_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as 17, but with a learning-rate of 5e-6 instead of 5e-5

24_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as 14, but with a learning-rate of 5e-6 instead of 5e-5

25_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as 18, but with a learning-rate of 5e-6 instead of 5e-5


26_arl:
(bad, used solver action for last action)
same as 14, but with aux_switch_ratio of 1

27_arl:
(bad, used solver action for last action)
same as 14, but with aux_switch_ratio of 100

28_arl:
(bad, used solver action for last action)
same as 14, but with internal scaling of 1 (used to see to try and compare differences between the ones that had the internal reward set by mistake)
a thing that is clear, is that using a too small internal reward is clearly what makes them collapse into making exceedingly small levels

29_arl:
rerun of 14, but with the last_action = solver_action bug fixed
(Poor results show that it might just be lerning faster now, since it seems like it has achieved similar results in 12 hours to 14_2 in 24 hours)

30_arl:
Using pretrained on simplified levels
same as 29, except using simplified levels
generator has internal reward scaled by 10
(simplified pretrained is 80.000.000, full pretrained is 70.000.000, the first time it had significantly good winrate, but not later to try and avoid overfitting. This is at least roughly, it is actually chosen a little after it has "plateaued")

31_arl:
same as 30, but internal reward scaled by 5


32_arl:
(produced many valid levels)
(validation bug, revalidate, on desktop, sunday evening, 32_2 has no validation bug)
same as 31, but using new version of the generator environment that fails the generator and gives negative reward if start and end constraints are not met (instead of repairing)
(reason: repairing level can have the same effect as the bug of the last action, essentially getting an external reward for an action that the generator did take, and thus not supply proper rewards for some actions)

33_arl:
(produced few valid levels)
(validation bug, revalidate, on desktop, sunday evening, 33_2 has no validation bug)
(weird outlier with high winrate, could be that it is just using the dummy level that is assigned in the beginning, if it never generates anything that works)
same as 32, but with only external reward (internal reward = 0)

34_arl:
same as 30, but with internal reward set to 0

35_arl:
(produced few valid levels)
same as 32, but with internal reward set to 1


36_arl (deprecated):
(are worse results of even being able to construct valid levels, a symptom of a moving target?)
Iterative training (uses same fail-based environment reward as 32, but if when training solver if not generating a proper level in 20 tries, it will use one of the ensemble levels)
generator has internal reward scaled by 5
solver plays on one level for 512 steps (but 512*10 since solver uses a vectorized environment with 10 envs) before switching back to generator which traines for 32*10 steps
Aux_switch set to 20, to makes it more in line with the ~1:10 ratio of the ARL-paper


37_arl (started late):
same as 36, but running generator training twice per solver training
Trains generator twice per time it trains solver (so 640 before switch, but resets aux every 320 still), as the solver trains 5120, and that makes it more in line with the ~1:10 ratio of the ARL-paper
(When the generator has not learned to generate valid levels, it will not impact the training of the solver much, and later on when it has learned it should increase generalizing in the solver, as it would earlier see more procedurally generated levels)
(So in all, it should not hurt the solver too much, while it could have big upsides for the generator)
(This is needed, as running the simulation is the thing taking the most time, and thus if not equalizing like this, I fear the solver would get much more time to train than the generator would, so it is in an attempt to make it more balanced, while still being able to compare it to 32)


38_arl (maybe):
same as 36, but with repairing if the level is invalid instead of using ensemble

Things to test:
Iterative training from scratch
Tweaking number of levels that the solver is trained on between generator training
Iterative training from the pretrained model
Different observation (and maybe accompanying reward) for the generator where it has an array representing the whole level instead of just the most recent slice

Test generalizability of the trained agent in the pretrained vs the ARL-trained one
