5_arl:
Smaller networks, ext/int reward scaling 1/20 and min+max level length in observation

6_arl:
learning rate 1e-5

7_arl:
Observation with int grid instead of single id

8_arl:
New reward structure using only simulation when done with generating level

9_arl:
Rerun of 5_arl, where loading is fixed to work with the new version

10_arl:
same as 8 but with internal = 10 and external = 1

11_arl:
same as 8, but internal 5 and external 1 (gonna be something different, this one didn't actually really work)

More things to try:
Smaller learning_rate (Done)
Observations using the tile-grid instead of the slice id (Done)
New reward structure running simulation on the terminal action instead of using a performance map
Different ratio between internal and external reward
Using vectorized generator environment
Using lower min-level-length

