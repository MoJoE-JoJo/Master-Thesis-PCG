5_arl:
Smaller networks, ext/int reward scaling 1/20 and min+max level length in observation

6_arl:
learning rate 1e-5

7_arl:
Observation with int grid instead of single id

8_arl:
New reward structure using only simulation when done with generating level

9_arl:
Rerun of 5_arl, where loading is fixed to work with the new version

10_arl:
same as 8 but with internal = 10 and external = 1

11_arl:
Binary reward structure. If all constraints met, is equal to solver return times aux, else it gets -2500 for each of the 4 unmet constraints
Min length has also been set to 4
It also switches between aux_inputs 10 times more often


More things to try:
Smaller learning_rate (Done)
Observations using the tile-grid instead of the slice id (Done)
New reward structure running simulation on the terminal action instead of using a performance map
Different ratio between internal and external reward
Using vectorized generator environment
Using lower min-level-length

