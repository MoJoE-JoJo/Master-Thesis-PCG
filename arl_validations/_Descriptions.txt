5_arl:
Smaller networks, ext/int reward scaling 1/20 and min+max level length in observation

6_arl:
learning rate 1e-5

7_arl:
Observation with int grid instead of single id

8_arl:
(bad, used solver action for last action)
New reward structure using only simulation when done with generating level

9_arl:
(bad, used solver action for last action)
Rerun of 5_arl, where loading is fixed to work with the new version

10_arl:
(bad, used solver action for last action)
same as 8 but with internal = 10 and external = 1

11_arl:
(bad, used solver action for last action)
Binary reward structure. If all constraints met, is equal to solver return times aux, else it gets -2500 for each of the 4 unmet constraints
Min length has also been set to 4
It also switches between aux_inputs 10 times more often (aux_switch 100)

12_arl:
(bad, used solver action for last action)
Same as 10, vectorized with 10 pcg environments that use the same solver environment (should be properly reset), and a min level length of 4

13_arl:
(bad, used solver action for last action)
same as 10, but min level length of 4

14_arl:
(bad, used solver action for last action)
Same as 10, but using an external reward based on mapping the winrate and aux-inputs of the agent to rewards
(So internal reward scaled by 10)
Also is unvectorized
Also has min-length of 10

15_arl:
(bad, used solver action for last action)
Same as 14, but instead of mapping winrate and aux-input to rewards it maps solver-agent avg-rewards and aux-input to external reward


16_arl:
(bad, used solver action for last action)
Same as 14_arl, but external reward scaled to 0.8 instead of 1, such it never can give a higher reward than getting a reward directly from the solver-agent

---------- Baseline experiments ----------
Experiments meant to find the baseline parameters and the edges of what, or whether it, is possible 
0_arl:
(bad, used solver action for last action, should not be a problem here)
Generator with winrate-map reward, but not trained at all (same as 14)

17_arl:
(bad, used solver action for last action)
Same as 14, winrate-reward map, but with only training with a single aux-input (aux=1)

18_arl:
(bad, used solver action for last action)
Same as 17, but with an internal reward of 0, to check whether it can learn based on the winrate with the current intervals between updates (winrate-map should correspond directly to the desired performance of the solver at specific aux-input values)
(There is no reason to run with aux=0.5 as still with aux 1 it does not reach the target and just reaches 0.5 winrate)

19_arl:
(bad, used solver action for last action)
Same as 18, but with updating the generator network every 16 steps instead of 32 steps

20_arl:
(bad, used solver action for last action)
Same as 18, but with updating the generator network every 64 steps instead of 32 steps

21_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as 17, but with updating the generator network every 16 steps instead of 32 steps

22_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as as	zxÂ½17, but with updating the generator network every 64 steps instead of 32 steps


23_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as 17, but with a learning-rate of 5e-6 instead of 5e-5

24_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as 14, but with a learning-rate of 5e-6 instead of 5e-5

25_arl:
(internal reward scaled by 1, by mistake)
(bad, used solver action for last action)
same as 18, but with a learning-rate of 5e-6 instead of 5e-5


26_arl:
(bad, used solver action for last action)
same as 14, but with aux_switch_ratio of 1

27_arl:
(bad, used solver action for last action)
same as 14, but with aux_switch_ratio of 100

28_arl:
(bad, used solver action for last action)
same as 14, but with internal scaling of 1 (used to see to try and compare differences between the ones that had the internal reward set by mistake)
a thing that is clear, is that using a too small internal reward is clearly what makes them collapse into making exceedingly small levels

29_arl:
rerun of 14, but with the last_action = solver_action bug fixed
(Poor results show that it might just be lerning faster now, since it seems like it has achieved similar results in 12 hours to 14_2 in 24 hours)

30_arl:
Using pretrained on simplified levels
same as 29, except using simplified levels
generator has internal reward scaled by 10
(simplified pretrained is 80.000.000, full pretrained is 70.000.000, the first time it had significantly good winrate, but not later to try and avoid overfitting. This is at least roughly, it is actually chosen a little after it has "plateaued")

31_arl:
same as 30, but internal reward scaled by 5


32_arl:
(produced many valid levels)
(validation bug, revalidate, on desktop, sunday evening, 32_2 has no validation bug)
same as 31, but using new version of the generator environment that fails the generator and gives negative reward if start and end constraints are not met (instead of repairing)
(reason: repairing level can have the same effect as the bug of the last action, essentially getting an external reward for an action that the generator did take, and thus not supply proper rewards for some actions)

33_arl:
(produced few valid levels)
(validation bug, revalidate, on desktop, sunday evening, 33_2 has no validation bug)
(weird outlier with high winrate, could be that it is just using the dummy level that is assigned in the beginning, if it never generates anything that works)
same as 32, but with only external reward (internal reward = 0)

34_arl:
same as 30, but with internal reward set to 0

35_arl:
(produced few valid levels)
same as 32, but with internal reward set to 1


36_arl (deprecated):
(are worse results of even being able to construct valid levels, a symptom of a moving target?, makes little sense, as invalid levels are not run with the solver. or could it be that it used aux=20? 
Could it be switching back and forth between training the generator and solver in some way? It would be expected that the generator would at least learn to generate valid levels in the same timeframe
It seems like for 37, it is better at learning to generate valid levels now. Maybe it was the aux-switching. I primarily think that it is the lesser amount of training that is probably working against it. But it seems like tensorboard is counting the number of steps in a weird way, so that 36 and 37 looks like they have trained for more steps in shorter time than 32 did, 
so I suspect that has something to do with training both the solver and the generator, and tensorboard might not be properly handling that, as tensorboard was not really meant for things to train the way it is here.)
Iterative training (uses same fail-based environment reward as 32, but if when training solver if not generating a proper level in 20 tries, it will use one of the ensemble levels)
generator has internal reward scaled by 5
solver plays on one level for 512 steps (but 512*10 since solver uses a vectorized environment with 10 envs) before switching back to generator which traines for 32*10 steps
Aux_switch set to 20, to makes it more in line with the ~1:10 ratio of the ARL-paper


37_arl (started late tuesday morning):
same as 36, but running generator training twice per solver training
Trains generator twice per time it trains solver (so 640 before switch, but resets aux every 320 still), as the solver trains 5120, and that makes it more in line with the ~1:10 ratio of the ARL-paper
(When the generator has not learned to generate valid levels, it will not impact the training of the solver much, and later on when it has learned it should increase generalizing in the solver, as it would earlier see more procedurally generated levels)
(So in all, it should not hurt the solver too much, while it could have big upsides for the generator)
(This is needed, as running the simulation is the thing taking the most time, and thus if not equalizing like this, I fear the solver would get much more time to train than the generator would, so it is in an attempt to make it more balanced, while still being able to compare it to 32)

(The extra generator steps seen in the iterative learning could maybe be due to running inference on the model for the solver and that increasing the number of steps executed, maybe only when tensorboard is set to logging, maybe for each time predict is called)


38_arl:
Change the observation-space, action-space, and internal reward fomulation. 
Observation will consist of length of level, aux-input, and 16x16 tile-grid. 
Action-space will be one of 17 (essentially height of ground, 0-15, and a flag 1x16 slice).
It will always start with the same 16x16 grid, and it will always end with the same 1x16 end slice.
Internal reward will just promote placing more tiles, to try and incentivise longer levels, while external reward will still be based on the solver.
Max length of level will be 19*16, after which it will place an end-slice (always placing end slice at the end, but level cannot be longer than 20*16, so it will overwrite final action if it is not the action for end slice).
Number of steps before update will be 400 (a little more than 1 levels if making them max level length, to have large trajectories to learn over, hoepefully decreasing instability. Doing 800, just made it take too long between updates, and it will usually also generate much shorter levels as it can also generate shorter levels than the max length).
It will not be iterative, but jsut use the pretrained for the solver.
Make new script for environment and the ARLPCG.
Internal reward will give 10 points per extra slice, but will give -100 if it is more than 4 higher than the most current slice
#Using avg_return reward + winrate, as avg_return should be better at indicating to the generator how far the solver got, and thus give more nuanced image of hwo good the level is, and the winrate will give concrete of how well the solver performs
Using only winrate reward, as that gives the most precise indicator of whether or not the level can be completed and how hard it is, though it also gives less frequent rewards, so it might take longer and be a bit harder to learn from it
Aux-switch if set to 5 instead of 10, due to time constraints, but it should still use update several times per aux input (it would take too long between the switches between logging and no logging, if it was 10, at least that is what was feared, and besides since so many factors are changed this can be changed too)
(the thing with measuring time between logs, is just to not have too many gigabytes of logs, so 5 was thought to be a good middleground)
Doing it only aux_switch 1, as it would otherwise take too lomng to run an iteration

39_arl:
same as 38, but even simpler, it will always just apply 160 actions
With levels of this length, I can also reduce the amount of time that the agents have to play to 15 seconds
Also set aux-switch back to 10, as it does not spend as much time on simulations now compared to actions of the policy

40_arl:
same as 39, but with a vectorized generator environment, with 5 generators (not 10, beacause of like computation time limitations with how the tensorboard logging is set up) (also trains for 5 times as long before switching aux-input, but that is beacause there are also 5 times the number of environments)
(not quite learning what it should, could be because of the static agent, not recognizing the "high-ground" generated by the generator)
(If 40 had followed the same trajectory as 41, then it might have worked better as it could have provided more stable solver)


41_arl:
Same as 40, but will do iterative learning instead of using pretrained.
The solver will train in the same way it has done up until now, no need to change that, as it has been proven to work.
(Using the method of 38, it would also never have to specifically handle the level generation, as it will always only be able to generate levels that are syntactically valid)
Also training solver for 5120 steps at a time, as it needs to train about 10 times more than the generator (actually trains it 51200 steps at a time, as the generator has the aux_switch of 10 and thus trains fro 4000 steps before switching, actually 20000 because it trains 4000 for each environment, but it is fine because in this formulation the granularity of steps are of more similar proportions compared to the ARL-PCG paper)
(If it looks like the solver does not get good enough on the distribution, as happened in 39 and 40, it could be that the ratio between training the solver and the generator is skewed too much towards the generator, and that it is simply because the solver has not had enough time during the experiment)
(potentially by doing some pretraining of one or the otehr agents, it could amke the learnign process more optimal, as the solver cannot really start learnign anything before the generator as acheived a certain minimum and makes completable levels)
(evaluated after 12 hours, saw the results were not good enough and that evaluation took very long, so decided to postspone evaluations and do them more infrequently)
(potential "flaw", solver does only have 15 seconds to complete the levels, which could be a little on the low side while it is learning, but the levels are shorter than most of the existing levels, it has at some points achieved to complete levels, and it was done in order to decrease the simulation-time fo the generators leading to a faster amortized step time, and thus it was surmised that it would be better to decrease the time the solver had from 30 seconds to 15 seconds)
(the rise and then subsequent fall can be due to them converging at first, and then the generator might have moved a bit too much, and thus starts generate levels in a new distribution, which the solver has not been trained over, and thus the solvers performance drops again as it needs to learn the levels of this new distribution, and the generators performance drops as its external reward drops)
(reason for solver somtimes getting an episode reward higher than ~2100, is that it is possible for it to play more than just one level during an episode)
(the bumping up and down when, i.e. both some episodes having ~1500 reward and some having ~4000 reward, could be that it does not learn to associate the auxiliary input with the difficulty, i.e. the desired winrate for specific aux-values, and that it might have learned to generate levels at one specific difficulty, and then it gets differing rewards based on what the aux-value is. Maybe with enough time this could be prevented, and it would be able to associate the desired winrate witht the aux-input)
(Potential improvement: When training solver, generate more than one level, and then just train a little on each level, such that instead of training 51200 on 1 level, it might train 5120 on each of 10 different levels, or something like that)
(cycles for solver where it changes from a long period of good to a long period of bad, and back and forth, could be that it forgets what is has previously learned, essentially overwriting the network of what has previously been learned, and then the generator changes the distribution it is generating in order to better fit to the aux-input. A potential improvement here could be to have shorter iterations, where we train generator and solver for shorter periods before switching, as this would give it less time to fit to a specific value/distribution, and less likely to overfit to one specific thing as it would change more often
essentially it can be compared with randomly choosing one of 4 levels to train on at each reset instead of first training on one level, and then the next, and then the next, which could lead to it forgetting what it learned about the first level, essentially to help prevent catastrophic forgetting, which could be a problem right now it seems) 

42_arl(maybe):
Same as 39, but using the pretrained solver trained on random stuff, to have a more generalized solver, and try to explore the role of the generalizability of the solver in the performance of the generator (though it should just change the form of the space it is searching through)


38_arl(maybe)(NOT DOING):
same as 37, but penalty for non-functioning of 1000 for missing start and another 1000 for missing end, making it more granular

39_arl (maybe)(NOT DOING):
same as 36, but with repairing if the level is invalid instead of using ensemble


----------
simplified solver baseline (definitely):
train solver with random levels between length 10 and 20 (1000 randomly generated levels)

ss trained with ensemble (baseline)
ss trained with generator (32_arl) (can a generator that works somewhat improve generalizability)
ss from 37/38/39 (is a solver that is trained iteratively even better at generalizing, even if the iterative-generator doesn't really work)

Things to test:
Iterative training from scratch
Tweaking number of levels that the solver is trained on between generator training
Iterative training from the pretrained model
Different observation (and maybe accompanying reward) for the generator where it has an array representing the whole level instead of just the most recent slice

Test generalizability of the trained agent in the pretrained vs the ARL-trained one

Maybe the RL has not have had long enough to converge, even though the graphs shows it, it is based on highly erratic data, and very unstable training, so it could be that maybe it would stabilize more later, or that it needs to be trained with lower learning-rate, but for much longer (there was not time to train with lower learning-rate for much longer)

Tensorboard logs are only to give an indication of how well something performs, but there has been some issues with how it works for this specific usecase, that has reduced their reliatbility and precision, as the actual final measure of how well it works, is to check how well it maps the range of aux-input to winrate.

Space could also be so simple now, that a low winrate is infeasible

Potentially, even for these simple spaces, more complex neural networks are needed for the generator policy, for it to even make something that is passable
Or maybe they just need to train longer

I am not gonna make variant that outright fails the generator action-sequence if a degenerate action is performed, as that means there needs to be a mechanism to handle what should happen when it fails, and that adds a lot of complexity to the iterative training (like if no valid levels are generated, then what is the solver trained on, or is it just skipped)